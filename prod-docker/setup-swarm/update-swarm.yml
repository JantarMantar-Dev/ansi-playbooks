---
- name: "Phase 1: Cluster Audit & Helper Discovery"
  hosts: swarm_managers
  become: yes
  gather_facts: no
  ignore_errors: yes
  tasks:
    - name: "AUDIT: CHECK FOR LEADER"
      # Added ManagerStatus to the end
      shell: "docker node ls --format '{{'{{'}}.Self{{'}}'}}|{{'{{'}}.Hostname{{'}}'}}|{{'{{'}}.Status{{'}}'}}|{{'{{'}}.Availability{{'}}'}}|{{'{{'}}.ID{{'}}'}}|{{'{{'}}.ManagerStatus{{'}}'}}'"
      register: swarm_audit
      changed_when: false
      failed_when: false

    - name: "LOG: Leader Audit Output"
      debug:
        msg: 
          - "{{ inventory_hostname }} > docker node ls"
          - "{{ swarm_audit.stdout_lines }}"
      when: swarm_audit.rc == 0

    - name: "AUDIT: Found Leader"
      set_fact:
        is_leader: true
        active_nodes: "{{ swarm_audit.stdout_lines | map('split', '|') | selectattr(2, 'equalto', 'Ready') | map(attribute=1) | list }}"
        down_nodes: "{{ swarm_audit.stdout_lines | map('split', '|') | selectattr(2, 'equalto', 'Down') | map(attribute=4) | list }}"
        # Check if the node listing itself is a Leader
        is_actual_leader: "{{ (swarm_audit.stdout_lines | select('match', '^true\\|.*\\|Leader$') | list | length) > 0 }}"
      when: 
        - swarm_audit.rc == 0
        # Check for 'true|' at the start of a line (Self=true), implying valid output
        - swarm_audit.stdout is search('^true\\|', multiline=True)

    # We only need ONE valid leader to source truth from.
    - name: "FACT: Register Leader Info"
      set_fact:
        swarm_leader_host: "{{ inventory_hostname }}"
        current_active_map: "{{ active_nodes }}"
        cleanup_ids: "{{ down_nodes }}"
      when: is_leader | default(false)

    - name: "TOKEN: Retrieve Tokens"
      shell: |
         echo "MANAGER:$(docker swarm join-token -q manager)"
         echo "WORKER:$(docker swarm join-token -q worker)"
      register: swarm_tokens
      when: is_leader | default(false)
      changed_when: false

    - name: "CHECK: Get Leader Tailscale IP"
      shell: "ip -4 addr show tailscale0 | awk '/inet / {print $2}' | cut -d/ -f1"
      register: leader_tailscale_ip
      when: 
        - is_leader | default(false)
        - use_tailscale | default(false) | bool
      changed_when: false
      failed_when: false

    - name: "FACT: Parse Tokens"
      set_fact:
        manager_token: "{{ swarm_tokens.stdout_lines[0].split(':')[1] }}"
        worker_token: "{{ swarm_tokens.stdout_lines[1].split(':')[1] }}"
        leader_ip: "{{ leader_tailscale_ip.stdout if (use_tailscale | default(false) | bool and leader_tailscale_ip.rc == 0 and leader_tailscale_ip.stdout != '') else ansible_host }}"
      when: is_leader | default(false)

- name: "Phase 2 & 3: Node Evaluation & Repair"
  hosts: all
  become: yes
  gather_facts: no
  vars:
    # Safely get facts from the identified leader
    leader_host: "{{ groups['swarm_managers'] | map('extract', hostvars) | selectattr('swarm_leader_host', 'defined') | map(attribute='inventory_hostname') | first | default('') }}"
    known_active_nodes: "{{ hostvars[leader_host]['current_active_map'] | default([]) }}"
    join_addr_manager: "{{ hostvars[leader_host]['leader_ip'] }}:2377"
    join_token_manager: "{{ hostvars[leader_host]['manager_token'] | default('') }}"
    join_token_worker: "{{ hostvars[leader_host]['worker_token'] | default('') }}"
  tasks:
    - name: "CHECK: Local Swarm Status"
      command: docker info --format '{{"{{"}}.Swarm.LocalNodeState{{"}}"}}'
      register: local_status
      changed_when: false

    - name: "CHECK: Get Tailscale IP"
      shell: "ip -4 addr show tailscale0 | awk '/inet / {print $2}' | cut -d/ -f1"
      register: tailscale_ip
      when: use_tailscale | default(false) | bool
      changed_when: false
      failed_when: false

    - name: "FAIL: Missing Tailscale IP"
      fail:
        msg: "SKIPPING: Tailscale requested but no IP found on tailscale0 for {{ inventory_hostname }}"
      when: 
        - use_tailscale | default(false) | bool
        - tailscale_ip.stdout == ""
      ignore_errors: yes
      register: tailscale_check

    - name: "FACT: Set Advertise Address"
      set_fact:
        swarm_advertise_addr: "{{ tailscale_ip.stdout if (use_tailscale | default(false) | bool and tailscale_ip.rc == 0 and tailscale_ip.stdout != '') else ansible_host }}"

    - name: "CHECK: Verify Advertise/Manager Address"
      shell: "docker info --format '{{'{{'}}.Swarm.RemoteManagers{{'}}'}}'"
      register: remote_managers_json
      changed_when: false
      failed_when: false
      check_mode: no

    # Use swarm_advertise_addr for comparison to catch Tailscale vs Public mismatches
    - name: "CHECK: Evaluate Address Mismatch"
      set_fact:
        advertise_addr_mismatch: "{{ '172.17.0.1' in remote_managers_json.stdout or (swarm_advertise_addr not in remote_managers_json.stdout) }}"
      when: remote_managers_json.rc == 0

    - name: "DECIDE: Determine Action"
      set_fact:
        start_action: >-
          {% if leader_host == '' %}
            ABORT_NO_LEADER
          {% elif inventory_hostname == leader_host and advertise_addr_mismatch | default(false) %}
            REPAIR_LEADER
          {% elif inventory_hostname == leader_host and force_leader_reinit | default(false) | bool %}
            REPAIR_LEADER
          {% elif inventory_hostname == leader_host %}
            LEADER_SKIP
          {% elif force_swarm_reinit | default(false) | bool %}
            REPAIR
          {% elif local_status.stdout != 'active' %}
            JOIN
          {% elif advertise_addr_mismatch | default(false) %}
            REPAIR
          {% elif inventory_hostname not in known_active_nodes %}
            REPAIR
          {% else %}
            HEALTHY_SKIP
          {% endif %}

    - name: "REPORT: Planned Action"
      debug:
        msg: 
          - "Node: {{ inventory_hostname }}"
          - "Status: {{ local_status.stdout }}"
          - "Current Advertise Addr (RemoteManagers): {{ remote_managers_json.stdout | default('Unknown') }}"
          - "Address Mismatch: {{ advertise_addr_mismatch | default(false) }}"
          - "Force Reinit: {{ force_swarm_reinit | default(false) }}"
          - "Action: {{ start_action | trim }}"

    - name: "FAIL: No Leader Found"
      fail:
        msg: "No healthy Swarm Leader found in inventory! Cannot proceed with repairs."
      when: (start_action | trim) == 'ABORT_NO_LEADER'

    - name: "REPAIR: Force Re-Init Leader"
      command: "docker swarm init --force-new-cluster --advertise-addr {{ swarm_advertise_addr }} --listen-addr {{ swarm_advertise_addr }}:2377"
      when: (start_action | trim) == 'REPAIR_LEADER'
      register: reinit_res

    - name: "LOG: Re-Init Output"
      debug:
        msg:
          - "{{ inventory_hostname }} > docker swarm init --force-new-cluster"
          - "{{ reinit_res.stdout_lines | default(['No output']) }}"
          - "{{ reinit_res.stderr_lines | default(['No error']) }}"
      when: reinit_res is changed

    - name: "REPAIR: Force Leave Broken/Phantom Node"
      command: docker swarm leave --force
      when: (start_action | trim) == 'REPAIR'
      register: leave_res
      ignore_errors: yes

    - name: "LOG: Leave Output"
      debug:
        msg:
          - "{{ inventory_hostname }} > docker swarm leave --force"
          - "{{ leave_res.stdout_lines | default(['No output']) }}"
          - "{{ leave_res.stderr_lines | default(['No error']) }}"
      when: (start_action | trim) == 'REPAIR'

    - name: "ACTION: Join as Manager"
      command: "docker swarm join --token {{ join_token_manager }} --advertise-addr {{ swarm_advertise_addr }} --listen-addr {{ swarm_advertise_addr }}:2377 {{ join_addr_manager }}"
      when: 
        - "'swarm_managers' in group_names"
        - ((start_action | trim) == 'JOIN') or ((start_action | trim) == 'REPAIR')
      register: join_man
      ignore_errors: yes

    - name: "LOG: Join Manager Output"
      debug:
        msg:
          - "{{ inventory_hostname }} > docker swarm join (manager)"
          - "{{ join_man.stdout_lines | default(['No output']) }}"
          - "{{ join_man.stderr_lines | default(['No error']) }}"
      when: join_man is changed

    - name: "RECOVERY: Force Leave if Already Swarm Member (Manager)"
      command: docker swarm leave --force
      when: 
        - join_man.failed is defined
        - join_man.failed
        - "'This node is already part of a swarm' in join_man.stderr"
      register: leave_recovery_man

    - name: "RECOVERY: Retry Join as Manager"
      command: "docker swarm join --token {{ join_token_manager }} --advertise-addr {{ swarm_advertise_addr }} --listen-addr {{ swarm_advertise_addr }}:2377 {{ join_addr_manager }}"
      when: 
        - leave_recovery_man is changed
      register: retry_join_man

    - name: "ACTION: Join as Worker"
      command: "docker swarm join --token {{ join_token_worker }} --advertise-addr {{ swarm_advertise_addr }} {{ join_addr_manager }}"
      when: 
        - "'swarm_workers' in group_names"
        - ((start_action | trim) == 'JOIN') or ((start_action | trim) == 'REPAIR')
      register: join_work
      ignore_errors: yes

    - name: "LOG: Join Worker Output"
      debug:
        msg:
          - "{{ inventory_hostname }} > docker swarm join (worker)"
          - "{{ join_work.stdout_lines | default(['No output']) }}"
          - "{{ join_work.stderr_lines | default(['No error']) }}"
      when: join_work is changed

    - name: "RECOVERY: Force Leave if Already Swarm Member (Worker)"
      command: docker swarm leave --force
      when: 
        - join_work.failed is defined
        - join_work.failed
        - "'This node is already part of a swarm' in join_work.stderr"
      register: leave_recovery_work

    - name: "RECOVERY: Retry Join as Worker"
      command: "docker swarm join --token {{ join_token_worker }} --advertise-addr {{ swarm_advertise_addr }} {{ join_addr_manager }}"
      when: 
        - leave_recovery_work is changed
      register: retry_join_work

- name: "Phase 4: Cleanup Ghost Nodes"
  hosts: swarm_managers
  become: yes
  gather_facts: no
  vars:
    # Only run on the leader we identified
    is_designated_leader: "{{ swarm_leader_host is defined }}"
  tasks:
    - name: "CLEANUP: Remove Down Nodes"
      command: "docker node rm {{ item }}"
      with_items: "{{ cleanup_ids | default([]) }}"
      when: is_designated_leader | default(false)
      ignore_errors: yes
      register: rm_res
      changed_when: rm_res.rc == 0

    - name: "LOG: Cleanup Output"
      debug:
        msg: 
          - "{{ inventory_hostname }} > docker node rm {{ item.item }}"
          - "{{ item.stdout_lines | default(['No output']) }}"
          - "{{ item.stderr_lines | default(['No error']) }}"
      with_items: "{{ rm_res.results }}"
      when: 
        - is_designated_leader | default(false)
        - item.changed

- name: "Phase 5: Verification"
  hosts: all
  become: yes
  gather_facts: no
  vars:
    leader_host: "{{ groups['swarm_managers'] | map('extract', hostvars) | selectattr('swarm_leader_host', 'defined') | map(attribute='inventory_hostname') | first | default('') }}"
  tasks:
    - name: "VERIFY: Check Address Advertisement"
      shell: "docker info --format '{{'{{'}}.Swarm.RemoteManagers{{'}}'}}'"
      register: verify_address
      ignore_errors: yes
      changed_when: false

    - name: "FACT: Determine Expected Leader Address"
      set_fact:
        expected_leader_addr: >-
          {{ 
            hostvars[leader_host]['leader_tailscale_ip']['stdout'] 
            if (use_tailscale | default(false) | bool and hostvars[leader_host]['leader_tailscale_ip'] is defined) 
            else hostvars[leader_host]['ansible_host'] 
          }}

    - name: "VERIFY: Compare Address"
      assert:
        that:
          - "'172.17.0.1' not in verify_address.stdout"
          - "expected_leader_addr in verify_address.stdout"
        fail_msg: "Address mismatch! Advertised: {{ verify_address.stdout }} Expected to contain: {{ expected_leader_addr }}"
        success_msg: "Address match validated: {{ verify_address.stdout }}"
      ignore_errors: yes

    - name: "LOG: Verification Result"
      debug:
        msg:
          - "Node: {{ inventory_hostname }}"
          - "Address Check: {{ 'PASSED' if verify_address.rc == 0 else 'FAILED' }}"
          - "Output: {{ verify_address.stdout }}"
      when: verify_address.rc is defined

- name: "Phase 6: Cluster State"
  hosts: swarm_managers
  become: yes
  gather_facts: no
  run_once: true
  tasks:
    - name: "VERIFY: Get Final Node List"
      command: docker node ls
      register: final_node_ls
      changed_when: false
      failed_when: false
    
    - name: "LOG: Final Cluster State"
      debug:
        msg: 
          - "FINAL CLUSTER STATE (from {{ inventory_hostname }}):"
          - "{{ final_node_ls.stdout_lines }}"
      when: final_node_ls.rc == 0

- name: "Phase 7: Detailed Summary Log"
  hosts: all
  become: yes
  gather_facts: no
  vars:
    # Safely get facts from the identified leader (re-calculated to ensure access)
    leader_host: "{{ groups['swarm_managers'] | map('extract', hostvars) | selectattr('swarm_leader_host', 'defined') | map(attribute='inventory_hostname') | first | default('') }}"
    join_addr_manager: "{{ hostvars[leader_host]['leader_ip'] }}:2377"
  tasks:
    - name: "CHECK: Get Tailscale IP (Summary)"
      shell: "ip -4 addr show tailscale0 | awk '/inet / {print $2}' | cut -d/ -f1"
      register: summary_tailscale_ip
      changed_when: false
      failed_when: false

    - name: "LOG: Detailed Node Status"
      debug:
        msg:
          - "---------------------------------------------------"
          - " Node: {{ inventory_hostname }}"
          - " Public IP: {{ ansible_host }}"
          - " Tailscale IP: {{ summary_tailscale_ip.stdout | default('Not Found') }}"
          - " Worker Swarm Advertise Addr: {{ hostvars[inventory_hostname]['swarm_advertise_addr'] | default('N/A (Skipped/Failed)') }}"
          - " Managers Address: {{ join_addr_manager }}"
          - " Status: {{ 'SKIPPED (No Tailscale IP)' if (use_tailscale | default(false) | bool and summary_tailscale_ip.stdout == '') else 'PROCESSED' }}"
          - "---------------------------------------------------"

